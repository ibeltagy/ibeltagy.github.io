I am a Co-founder and the Chief Scientist of [spiffy.ai](https://spiffy.ai), a startup building intelligent systems that bring out the best in human capabilities.

I am a Lead Research Scientist at [Allen Institute for AI (AI2)](https://allenai.org/) in the [AllenNLP team](https://allennlp.org/), and the Research Lead of the OLMo project. OLMo aimes to build "state-of-the-art", "truly-open" LLMs to narrow the gap between open and proprietary research. My role started with ideation and feasibility studies before the project's inception, continuing through to the present where we now have a mature, large, corss-team project.

I received my PhD in Computer Science from the [University of Texas at Austin](https://www.cs.utexas.edu/) working with [Ray Mooney](https://www.cs.utexas.edu/~mooney/) and [Katrin Erk](http://www.katrinerk.com/). 

For the list of publications,
please check my [Semantic Scholar](https://www.semanticscholar.org/author/Iz-Beltagy/46181066?sort=pub-date) or [Google Scholar](https://scholar.google.com/citations?hl=en&user=jkV6H1gAAAAJ&view_op=list_works&sortby=pubdate) pages.

### Select Projects

- **OLMo** A large cross-teams project at AI2 aiming to train truly open SOTA LLMs to narrow the gap between open and proprietary LLM research. [https://allenai.org/olmo](https://allenai.org/olmo)

- **TÜLU-1 and TÜLU-2** Instruction-tuned and DPO-based models outperforming llama-2 on a wide set of tasks. [Paper [1](https://arxiv.org/abs/2306.04751), [2](https://arxiv.org/abs/2311.10702)], [[Code](https://github.com/allenai/open-instruct)].

- **DOLMA** A state-of-the-art pretraining dataset of 3T tokens. [[Blog](https://blog.allenai.org/dolma-3-trillion-tokens-open-llm-corpus-9a0ff4b8da64)]. [[Data and Toolkit](https://github.com/allenai/dolma)].

- **BLOOM** - the open-source LLM from the BigScience project, which co-led its architecture and scaling group. [Architecture papers [1](https://aclanthology.org/2022.findings-emnlp.54/), [2](https://proceedings.mlr.press/v162/wang22u/wang22u.pdf)], [[Overview paper](https://arxiv.org/abs/2211.05100)], [[Model](https://huggingface.co/bigscience/bloom)]

- **Can LMs learn "meaning"** - TACL2022 paper led by Zhaofeng. [[Paper](https://arxiv.org/abs/2210.07468)]

- **Z-ICL** - zero-shot ICL with pseudo-demonstrations. [[Paper](https://arxiv.org/abs/2212.09865)]

- **Staged Training** - incrementally growing model size during pretraining. [[Paper](https://arxiv.org/abs/2203.06211)], [[Code](https://github.com/allenai/staged-training)]

- **FLEX** - principles of zero/few-shot NLP evaluation. [[Paper](https://arxiv.org/abs/2107.07170)] [[Leaderboard](https://leaderboard.allenai.org/flex/submissions/public)] [[Baseline model](https://github.com/allenai/unifew)]

- **CDLM** - a pretrained language model for cross-document proecssing. [[Code and Pretrained Model](https://github.com/aviclu/CDLM)]

- **MS2** - a large scale biomedical multi-document summarization dataset. [[Code and Dataset](https://github.com/allenai/ms2)]

- **LongformerEncoderDecoder (LED)** - a pretrained transformer for long-document generation tasks. [[Code and Pretrained model](https://github.com/allenai/longformer)]

- **Longformer** - a BERT-like model for long documents. [[Code and Pretrained model](https://github.com/allenai/longformer)]

- **SPECTER** - a citation-informed embedding model for scintific documents.  [[Code, Data and Pretrained Model](https://github.com/allenai/specter)]

- **SciSpacy** - a Spacy pipeline for scientific documents. [[Code](https://github.com/allenai/scispacy)]

- **SciBERT** - a BERT model for scientific documents. [[Code, Data, and Pretrained model](https://github.com/allenai/scibert)]

### Updates

- [5/2024] OLMo won the **Innovation of the Year** awared at GeekWire Awards! I had the privilege of accepting the award and delivering the speech. [[GeekWire](https://www.geekwire.com/2024/geekwire-awards-2024-revealed-winners-bask-in-momentum-of-ai-and-potential-of-region/)]. [[Twitter](https://x.com/allen_ai/status/1788999117747941522)].

- [2/2024] **OLMo** is out. [[Blog](https://blog.allenai.org/olmo-open-language-model-87ccfc95f580)]. [[Paper](https://arxiv.org/abs/2402.00838)].

- [12/2023] Giving an invited talk at the [NewSumm, EMNLP 2023](https://newsumm.github.io/2023/) workshop.

- [11/2023] Our TÜLU-2 paper is out. [[Paper](https://arxiv.org/abs/2311.10702)], [[Code](https://github.com/allenai/open-instruct)].

- [11/2023] **Raised seed funding for my startup, [https://spiffy.ai](https://spiffy.ai)**.

- [10/2023] Gave a talk at the [ELLIS LLM symposium](https://sites.google.com/view/ellisfms2023) about OLMo and open language models. [[Slides](https://docs.google.com/presentation/d/1WLTudJGx-dOFfqVn-fYyh4HVjlPofWw7E5yJHpRMa6A/edit)].

- [10/2023] Invited to serve as an Area Chair for EACL 2024.

- [10/2023] Gave a guest lecture at [Arman's Foundation Models course](https://yale-nlp.github.io/cpsc488/schedule/).

- [9/2023] Our TÜLU-1 paper is accepted for publishing at NeurIPS 2023 Datasets and Benchmarks Track. [[Paper](https://arxiv.org/abs/2306.04751)], [[Code](https://github.com/allenai/open-instruct)].

- [8/2023] The **DOLMA** dataset is out. [[Blog](https://blog.allenai.org/dolma-3-trillion-tokens-open-llm-corpus-9a0ff4b8da64)]. [[Data and Toolkit](https://github.com/allenai/dolma)].

- [8/2023] Serving as an Area Chair for EMNLP 2023.

- [7/2023] Promoted to **Lead Research Scientist** at AI2.

- [7/2023] At ACL, 2023 in Toronto. Collaborators are presenting three papers [[1](https://arxiv.org/abs/2212.09865)], [[2](https://aclanthology.org/2023.acl-long.454/)], [[3](https://arxiv.org/abs/2210.07468)].

- [6/2023] An #NLPHighlights podcast about OLMo and open LLMs. [Link](https://twitter.com/allen_ai/status/1674794167015452673).

- [5/2023] Joined the panel discussion at the [Playground RoboChat event](https://twitter.com/i_beltagy/status/1658504613438558211) to talk about AI and the tradeoff between scale and efficiency.

- [5/2023] [Rabeeh](https://scholar.google.com/citations?user=buoHMDMAAAAJ&hl=en)'s internship on diffusion is on arxiv. [TESS: Text-to-Text Self-Conditioned Simplex Diffusion](https://arxiv.org/abs/2305.08379).

- [3/2023] **Research Lead of the [OLMo](https://allenai.org/olmo) project** aiming to train "truly open" "SOTA" LLMs.

- [2/2023] AI2 was offered 2M GPU hours from [LUMI](https://www.csc.fi/en/lumi), thanks to [Dirk](https://www.linkedin.com/in/mechanicaldirk/)'s efforts.

- [1-3/2023] Conversations inside AI2 to discuss the necessity and feasibility of training our own LLMs.

- [12/2022] At EMNLP in Abu Dhabi. Collaborators are presenting four papers [[1](https://aclanthology.org/2022.emnlp-main.300/)], [[2](https://aclanthology.org/2022.findings-emnlp.54/)], [[3](https://aclanthology.org/2022.findings-emnlp.347/)], [[4](https://aclanthology.org/2022.gem-1.51/)].

- [7/2022] Gave a talk to [The UKP Lab](https://www.informatik.tu-darmstadt.de/ukp/ukp_home/staff_ukp/index.en.jsp) at the Technical University of Darmstadt. [[Slides](https://github.com/ibeltagy/ibeltagy.github.io/raw/master/assets/UKP_BeyondParagraphsLanguageModelingofLongSequences.pdf)]

- [6/2022] Invited to serve as AC for EMNLP 2022 for four tracks.

- [6/2022] Serving as a standing reviewer for [TACL](https://transacl.org/ojs/index.php/tacl/about/editorialTeam).

- [5/2022] An average of ~500 attended our tutorial, [Zero- and Few-Shot NLP with Pretrained Language Models](https://github.com/allenai/acl2022-zerofewshot-tutorial) at ACL 2022. [Slides](https://github.com/allenai/acl2022-zerofewshot-tutorial). [Videos](https://underline.io/events/284/sessions?eventSessionId=10748) (require ACL 2022 registration with Underline). [Arman presenting part 6 of the tutorial](https://raw.githubusercontent.com/ibeltagy/ibeltagy.github.io/master/assets/tutorial_acl2022.jpeg).

- [5/2022] Gave an invited talk with Julien Launay at the [Challenges & Perspectives in Creating Large Language Models](https://bigscience.huggingface.co/acl-2022) workshop, co-located with ACL 2022. [[Slides](https://raw.githubusercontent.com/ibeltagy/ibeltagy.github.io/master/assets/acl2022.pdf?raw=true)].

- [4/2022] Gave an invited talk at the [Georgia Tech NLP seminar](https://sites.google.com/view/nlpseminar/home) about Efficient Scaling of LM pretraining. [[Slides](https://raw.githubusercontent.com/ibeltagy/ibeltagy.github.io/master/assets/EfficientScalingOfLMpretraining.pdf?raw=true)]

- [4/2022] Our paper [What Language Model to Train if You Have One Million GPU Hours?](https://openreview.net/pdf?id=rI7BL3fHIZq) is now available. It outlines the design and training setup of the [BigScience language model](https://bigscience.huggingface.co/).

- [4/2022] Two papers accepted at Findings of NAACL 2022, [LongChecker](https://arxiv.org/abs/2112.01640) and [Few-shot Self-Rationalization](https://arxiv.org/abs/2111.08284).

- [3/2022] Our Staged Training paper is now available on arxiv. [[Paper](https://arxiv.org/abs/2203.06211)] [[Code](https://github.com/allenai/staged-training)]

- [3/2022] Training of the [BigScience language model](https://bigscience.huggingface.co/) has started. The model is based on our paper [here](https://openreview.net/pdf?id=rI7BL3fHIZq); an effor by the Architecture and Scaling group that I am [co-chairing](https://twitter.com/BigscienceW/status/1505835083994959874).

- [12/2022] Wen Xiao, my intern with Arman won [Intern of the Year](https://allenai.org/outstanding-interns) award at AI2. Her [paper](https://arxiv.org/abs/2110.08499) on multi-document summarization has been accepted for publication at ACL 2022.

- [12/2022] Our tutorial [Zero- and Few-Shot NLP with Pretrained Language Models](https://www.2022.aclweb.org/tutorials) has been accepted to appear at ACL 2022. 

- [10/2021] Serving as an Action Editor for [ACL Rolling Review](https://aclrollingreview.org/people)

- [9/2021] Our paper, [FLEX](https://arxiv.org/abs/2107.07170) has been accepted at NeurIPS 2021.

- [9/2021] Our paper, [SciCo](https://arxiv.org/abs/2104.08809) won [**outstanding paper award at AKBC 2021**](https://www.akbc.ws/2021/awards/).

- [8/2021] Two EMNLP 2021 accepted papers; [MS2](https://arxiv.org/abs/2104.06486) at the main conference and [CDLM](https://arxiv.org/abs/2101.00406) in the Findings of EMNLP.

- [6/2021] Chairing the Architecture and Scaling group at the [BigScience](https://bigscience.huggingface.co/) project aiming to train a 200B-parameter multilingual language model. [[Recent slides](https://raw.githubusercontent.com/ibeltagy/ibeltagy.github.io/master/assets/big_science_episode_2.pdf?raw=true)]. [[Weekly meeting calendar invite](https://calendar.google.com/event?action=TEMPLATE&tmeid=N2w1aTNxMnVpM2o1MHNpcWJiamdoMnUwMThfMjAyMTEwMDZUMTQwMDAwWiBwa29zb2puamc0bm8ycDc0bGk2NmVvZmxxZ0Bn&tmsrc=pkosojnjg4no2p74li66eoflqg%40group.calendar.google.com&scp=ALL)].

- [5/2021] Joined the AllenNLP team at AI2.

- [3/2021] Serving as an area chair for the ML for NLP track at EMNLP 2021.

- [1/2021] Promoted to Senior Research Scientist at AI2.

- [12/2020] Selected as an Outstanding Reviewer for [EMNLP 2020](https://www.aclweb.org/anthology/2020.emnlp-main.0.pdf).

- [12/2020] LongformerEncoderDecoder (LED) is out. [[Paper](https://arxiv.org/abs/2004.05150)] [[Code and Pretrained model](https://github.com/allenai/longformer)]

- [12/2020] Our tutorial "Beyond Paragraphs: NLP for Long Sequences" has been accepted to appear at NAACL 2021.

- [11/2020] Co-organizing The Second Workshop on Scholarly Document Processing [(SDP 2021)](https://ornlcda.github.io/SDProc/2021/index.html).

- [10/2020] Serving as an area chair for the Sentence-level Semantics and Textual Inference track at ACL 2021.

- [10/2020] Serving as an area chair for the Sentence-level Semantics and Textual Inference track at NAACL 2021.

- [9/2020] Serving as a publication co-chair for [NAACL 2021](https://2021.naacl.org/organization).

- [7/2020] Our paper, [Don't Stop Pretraining](https://arxiv.org/abs/2004.10964), won an [**honorary mention at ACL 2020**](https://acl2020.org/blog/ACL-2020-best-papers/).

- [6/2020] Serving as a standing reviewer of Computational Linguistics (CL) journal.

- [6/2020] Gave a talk about **Longformer** to UW NLP students [[Slides](https://raw.githubusercontent.com/ibeltagy/ibeltagy.github.io/master/assets/longformer-slides.pdf?raw=true)]

- [6/2020] **Longformer** is now integrated into the [huggingface repo](https://huggingface.co/allenai/longformer-base-4096)

- [5/2020] **SciBERT** has been downloaded more than [20,000 times](https://huggingface.co/allenai/scibert_scivocab_uncased) in the last [30 days](https://raw.githubusercontent.com/ibeltagy/ibeltagy.github.io/master/assets/scibert.png?raw=true)

- [4/2020] **Longformer** is [out](https://twitter.com/i_beltagy/status/1254907063492206592)

- [4/2020] 3/3 papers accepted at ACL 2020 [[1](https://github.com/allenai/scirex)], [[2](https://github.com/allenai/specter)], [[3](https://github.com/allenai/dont-stop-pretraining)]

- [3/2020] Co-organizing the SciNLP workshop. Check [scinlp.org](http://scinlp.org/)
