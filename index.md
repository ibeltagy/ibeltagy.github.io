I am a Senior Research Scientist at [Allen Institute for Artificial Intelligence (AI2)](https://allenai.org/) in the [AllenNLP team](https://allennlp.org/).
I received my PhD in Computer Science from the [University of Texas at Austin](https://www.cs.utexas.edu/) working with [Ray Mooney](https://www.cs.utexas.edu/~mooney/) and [Katrin Erk](http://www.katrinerk.com/). 

For the list of publications, 
please check my [Semantic Scholar](https://www.semanticscholar.org/author/Iz-Beltagy/46181066?sort=pub-date) or [Google Scholar](https://scholar.google.com/citations?hl=en&user=jkV6H1gAAAAJ&view_op=list_works&sortby=pubdate) pages.


### Select Recent Projects

- **BLOOM** - the open-source LLM from the BigScience project, which co-led its architecture and scaling group. [Architecture papers [1](https://aclanthology.org/2022.findings-emnlp.54/), [2](https://proceedings.mlr.press/v162/wang22u/wang22u.pdf)], [[Overview paper](https://arxiv.org/abs/2211.05100)], [[Model](https://huggingface.co/bigscience/bloom)]

- **Can LMs learn "meaning"** - TACL2022 paper led by Zhaofeng. [[Paper](https://arxiv.org/abs/2210.07468)]

- **Z-ICL** - zero-shot ICL with pseudo-demonstrations. [[Paper](https://arxiv.org/abs/2212.09865)]

- **Staged Training** - incrementally growing model size during pretraining. [[Paper](https://arxiv.org/abs/2203.06211)], [[Code](https://github.com/allenai/staged-training)]

- **FLEX** - principles of zero/few-shot NLP evaluation. [[Paper](https://arxiv.org/abs/2107.07170)] [[Leaderboard](https://leaderboard.allenai.org/flex/submissions/public)] [[Baseline model](https://github.com/allenai/unifew)]

- **CDLM** - a pretrained language model for cross-document proecssing. [[Code and Pretrained Model](https://github.com/aviclu/CDLM)]

- **MS2** - a large scale biomedical multi-document summarization dataset. [[Code and Dataset](https://github.com/allenai/ms2)]

- **LongformerEncoderDecoder (LED)** - a pretrained transformer for long-document generation tasks. [[Code and Pretrained model](https://github.com/allenai/longformer)]

- **Longformer** - a BERT-like model for long documents. [[Code and Pretrained model](https://github.com/allenai/longformer)]

- **SPECTER** - a citation-informed embedding model for scintific documents.  [[Code, Data and Pretrained Model](https://github.com/allenai/specter)]

- **SciSpacy** - a Spacy pipeline for scientific documents. [[Code](https://github.com/allenai/scispacy)]

- **SciBERT** - a BERT model for scientific documents. [[Code, Data, and Pretrained model](https://github.com/allenai/scibert)]

### Updates

- [10/2023] Gave a talk at the [ELLIS LLM symposium](https://sites.google.com/view/ellisfms2023) about OLMo and open language models. [Slides](https://docs.google.com/presentation/d/1WLTudJGx-dOFfqVn-fYyh4HVjlPofWw7E5yJHpRMa6A/edit).

- [10/2023] Invited to serve as an Area Chair for EACL 2024.


- [8/2023] Serving as an Area Chair for EMNLP 2023.


- [7/2023] Promoted to **Lead Research Scientist** at AI2.

- [7/2023] At ACL, 2023 in Toronto. Collaborators are presenting three papers [1](https://arxiv.org/abs/2212.09865), [2](https://aclanthology.org/2023.acl-long.454/), [3](https://arxiv.org/abs/2210.07468).


- [5/2023] [Rabeeh](https://scholar.google.com/citations?user=buoHMDMAAAAJ&hl=en)'s internship on diffusion is on arxiv. [TESS: Text-to-Text Self-Conditioned Simplex Diffusion](https://arxiv.org/abs/2305.08379).

- [12/2022] At EMNLP in Abu Dhabi. Collaborators are presenting four papers [1](https://aclanthology.org/2022.emnlp-main.300/), [2](https://aclanthology.org/2022.findings-emnlp.54/), [3](https://aclanthology.org/2022.findings-emnlp.347/), [4](https://aclanthology.org/2022.gem-1.51/).

- [7/2022] Gave a talk to [The UKP Lab](https://www.informatik.tu-darmstadt.de/ukp/ukp_home/staff_ukp/index.en.jsp) at the Technical University of Darmstadt. [[Slides](https://github.com/ibeltagy/ibeltagy.github.io/raw/master/assets/UKP_BeyondParagraphsLanguageModelingofLongSequences.pdf)]

- [6/2022] Invited to serve as AC for EMNLP 2022 for four tracks.

- [6/2022] Serving as a standing reviewer for [TACL](https://transacl.org/ojs/index.php/tacl/about/editorialTeam).

- [5/2022] An average of ~500 attended our tutorial, [Zero- and Few-Shot NLP with Pretrained Language Models](https://github.com/allenai/acl2022-zerofewshot-tutorial) at ACL 2022. [Slides](https://github.com/allenai/acl2022-zerofewshot-tutorial). [Videos](https://underline.io/events/284/sessions?eventSessionId=10748) (require ACL 2022 registration with Underline). [Arman presenting part 6 of the tutorial](https://raw.githubusercontent.com/ibeltagy/ibeltagy.github.io/master/assets/tutorial_acl2022.jpeg).

- [5/2022] Gave an invited talk with Julien Launay at the [Challenges & Perspectives in Creating Large Language Models](https://bigscience.huggingface.co/acl-2022) workshop, co-located with ACL 2022. [[Slides](https://raw.githubusercontent.com/ibeltagy/ibeltagy.github.io/master/assets/acl2022.pdf?raw=true)].

- [4/2022] Gave an invited talk at the [Georgia Tech NLP seminar](https://sites.google.com/view/nlpseminar/home) about Efficient Scaling of LM pretraining. [[Slides](https://raw.githubusercontent.com/ibeltagy/ibeltagy.github.io/master/assets/EfficientScalingOfLMpretraining.pdf?raw=true)]

- [4/2022] Our paper [What Language Model to Train if You Have One Million GPU Hours?](https://openreview.net/pdf?id=rI7BL3fHIZq) is now available. It outlines the design and training setup of the [BigScience language model](https://bigscience.huggingface.co/).

- [4/2022] Two papers accepted at Findings of NAACL 2022, [LongChecker](https://arxiv.org/abs/2112.01640) and [Few-shot Self-Rationalization](https://arxiv.org/abs/2111.08284).

- [3/2022] Our Staged Training paper is now available on arxiv. [[Paper](https://arxiv.org/abs/2203.06211)] [[Code](https://github.com/allenai/staged-training)]

- [3/2022] Training of the [BigScience language model](https://bigscience.huggingface.co/) has started. The model is based on our paper [here](https://openreview.net/pdf?id=rI7BL3fHIZq); an effor by the Architecture and Scaling group that I am [co-chairing](https://twitter.com/BigscienceW/status/1505835083994959874).

- [12/2022] Wen Xiao, my intern with Arman won [Intern of the Year](https://allenai.org/outstanding-interns) award at AI2. Her [paper](https://arxiv.org/abs/2110.08499) on multi-document summarization has been accepted for publication at ACL 2022.

- [12/2022] Our tutorial [Zero- and Few-Shot NLP with Pretrained Language Models](https://www.2022.aclweb.org/tutorials) has been accepted to appear at ACL 2022. 

- [10/2021] Serving as an Action Editor for [ACL Rolling Review](https://aclrollingreview.org/people)

- [9/2021] Our paper, [FLEX](https://arxiv.org/abs/2107.07170) has been accepted at NeurIPS 2021.

- [9/2021] Our paper, [SciCo](https://arxiv.org/abs/2104.08809) won [**outstanding paper award at AKBC 2021**](https://www.akbc.ws/2021/awards/).

- [8/2021] Two EMNLP 2021 accepted papers; [MS2](https://arxiv.org/abs/2104.06486) at the main conference and [CDLM](https://arxiv.org/abs/2101.00406) in the Findings of EMNLP.

- [6/2021] Chairing the Architecture and Scaling group at the [BigScience](https://bigscience.huggingface.co/) project aiming to train a 200B-parameter multilingual language model. [[Recent slides](https://raw.githubusercontent.com/ibeltagy/ibeltagy.github.io/master/assets/big_science_episode_2.pdf?raw=true)]. [[Weekly meeting calendar invite](https://calendar.google.com/event?action=TEMPLATE&tmeid=N2w1aTNxMnVpM2o1MHNpcWJiamdoMnUwMThfMjAyMTEwMDZUMTQwMDAwWiBwa29zb2puamc0bm8ycDc0bGk2NmVvZmxxZ0Bn&tmsrc=pkosojnjg4no2p74li66eoflqg%40group.calendar.google.com&scp=ALL)].

- [5/2021] Joined the AllenNLP team at AI2.

- [3/2021] Serving as an area chair for the ML for NLP track at EMNLP 2021.

- [1/2021] Promoted to Senior Research Scientist at AI2.

- [12/2020] Selected as an Outstanding Reviewer for [EMNLP 2020](https://www.aclweb.org/anthology/2020.emnlp-main.0.pdf).

- [12/2020] LongformerEncoderDecoder (LED) is out. [[Paper](https://arxiv.org/abs/2004.05150)] [[Code and Pretrained model](https://github.com/allenai/longformer)]

- [12/2020] Our tutorial "Beyond Paragraphs: NLP for Long Sequences" has been accepted to appear at NAACL 2021.

- [11/2020] Co-organizing The Second Workshop on Scholarly Document Processing [(SDP 2021)](https://ornlcda.github.io/SDProc/2021/index.html).

- [10/2020] Serving as an area chair for the Sentence-level Semantics and Textual Inference track at ACL 2021.

- [10/2020] Serving as an area chair for the Sentence-level Semantics and Textual Inference track at NAACL 2021.

- [9/2020] Serving as a publication co-chair for [NAACL 2021](https://2021.naacl.org/organization).

- [7/2020] Our paper, [Don't Stop Pretraining](https://arxiv.org/abs/2004.10964), won an [**honorary mention at ACL 2020**](https://acl2020.org/blog/ACL-2020-best-papers/).

- [6/2020] Serving as a standing reviewer of Computational Linguistics (CL) journal.

- [6/2020] Gave a talk about **Longformer** to UW NLP students [[Slides](https://raw.githubusercontent.com/ibeltagy/ibeltagy.github.io/master/assets/longformer-slides.pdf?raw=true)]

- [6/2020] **Longformer** is now integrated into the [huggingface repo](https://huggingface.co/allenai/longformer-base-4096)

- [5/2020] **SciBERT** has been downloaded more than [20,000 times](https://huggingface.co/allenai/scibert_scivocab_uncased) in the last [30 days](https://raw.githubusercontent.com/ibeltagy/ibeltagy.github.io/master/assets/scibert.png?raw=true)

- [4/2020] **Longformer** is [out](https://twitter.com/i_beltagy/status/1254907063492206592)

- [4/2020] 3/3 papers accepted at ACL 2020 [[1](https://github.com/allenai/scirex)], [[2](https://github.com/allenai/specter)], [[3](https://github.com/allenai/dont-stop-pretraining)]

- [3/2020] Co-organizing the SciNLP workshop. Check [scinlp.org](http://scinlp.org/)
